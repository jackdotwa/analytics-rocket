{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a single tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soybean = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-large.data',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "307 cases, 35 variables (observation on plants with some climatic variables) and 19 classes (soybean diseases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "1. date: april,may,june,july,august,september,october,?.\n",
    "2. plant-stand: normal,lt-normal,?.\n",
    "3. precip: lt-norm,norm,gt-norm,?.\n",
    "4. temp: lt-norm,norm,gt-norm,?.\n",
    "5. hail: yes,no,?.\n",
    "6. crop-hist: diff-lst-year,same-lst-yr,same-lst-two-yrs,\n",
    "same-lst-sev-yrs,?.\n",
    "7. area-damaged: scattered,low-areas,upper-areas,whole-field,?.\n",
    "8. severity: minor,pot-severe,severe,?.\n",
    "9. seed-tmt: none,fungicide,other,?.\n",
    "10. germination: 90-100%,80-89%,lt-80%,?.\n",
    "11. plant-growth: norm,abnorm,?.\n",
    "12. leaves: norm,abnorm.\n",
    "13. leafspots-halo: absent,yellow-halos,no-yellow-halos,?.\n",
    "14. leafspots-marg: w-s-marg,no-w-s-marg,dna,?.\n",
    "15. leafspot-size: lt-1/8,gt-1/8,dna,?.\n",
    "16. leaf-shread: absent,present,?.\n",
    "17. leaf-malf: absent,present,?.\n",
    "18. leaf-mild: absent,upper-surf,lower-surf,?.\n",
    "19. stem: norm,abnorm,?.\n",
    "20. lodging: yes,no,?.\n",
    "21. stem-cankers: absent,below-soil,above-soil,above-sec-nde,?.\n",
    "22. canker-lesion: dna,brown,dk-brown-blk,tan,?.\n",
    "23. fruiting-bodies: absent,present,?.\n",
    "24. external decay: absent,firm-and-dry,watery,?.\n",
    "25. mycelium: absent,present,?.\n",
    "26. int-discolor: none,brown,black,?.\n",
    "27. sclerotia: absent,present,?.\n",
    "28. fruit-pods: norm,diseased,few-present,dna,?.\n",
    "29. fruit spots: absent,colored,brown-w/blk-specks,distort,dna,?.\n",
    "30. seed: norm,abnorm,?.\n",
    "31. mold-growth: absent,present,?.\n",
    "32. seed-discolor: absent,present,?.\n",
    "33. seed-size: norm,lt-norm,?.\n",
    "34. shriveling: absent,present,?.\n",
    "35. roots: norm,rotted,galls-cysts,?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://archive.ics.uci.edu/ml/datasets/soybean+(small)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all \"?\" in table with NaN which will be replaced by the rounded column mean after each train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use= df_soybean\n",
    "df_to_use = df_to_use.replace('?', np.NaN)\n",
    "df_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If replace_after_split is False, the error rates are closer the the paper's result\n",
    "### The paper did not explicitly state when to replace missing values, \n",
    "### but it seems like best practice is to replace missing values after the train-test split.\n",
    "replace_after_split = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_after_split==False:\n",
    "    for i in range(1,36):\n",
    "         df_to_use[i] = df_to_use[i].fillna(round(df_to_use[i].dropna().astype(float).mean(),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Y is the category column and X is the matrix with all other columns\n",
    "X = df_to_use.values[:, 1:36]\n",
    "Y = df_to_use.values[:,0]#.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Viewing the Classification Column mapped to values\n",
    "df_to_use.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number of unique classes (Y values)\n",
    "#df_to_use.values[:,0].unique()\n",
    "print(len(df_to_use[0].unique()))\n",
    "df_to_use[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max = 100 # length of matrix (number repeats)\n",
    "j_max = 50 # width of matrix (bootstrap replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_0 is the base (without bagging)\n",
    "cases = len(df_to_use)\n",
    "err_0 = np.zeros((i_max,1)) #ToDo just confirm\n",
    "for i in range(i_max):\n",
    "    \n",
    "    for j in range(1):\n",
    "        print(i, j, end=\"\\r\")\n",
    "        df_to_use_sample = df_to_use\n",
    "        X = df_to_use_sample.values[:, 1:35]\n",
    "        Y = df_to_use_sample.values[:, 0]#.astype('category')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3) # 70% training and 30% test\n",
    "        X_train = X_train.astype(float)\n",
    "        X_test = X_test.astype(float)\n",
    "        # replace missing values with rounded mean (after train/test split)\n",
    "        # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "        col_means = np.nanmean(X_train, axis=0)\n",
    "        inds = np.where(np.isnan(X_train))\n",
    "        X_train[inds] = np.take(col_means, inds[1])\n",
    "        ### Rounding, because all values are categorical, not numerical\n",
    "        col_means = np.round(np.nanmean(X_test, axis=0))\n",
    "        inds = np.where(np.isnan(X_test))\n",
    "        X_test[inds] = np.take(col_means, inds[1])\n",
    "        \n",
    "        # Create Decision Tree classifer object\n",
    "        clf = DecisionTreeClassifier()\n",
    "        # Train Decision Tree Classifer\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        #Predict the response for test dataset\n",
    "        y_pred = clf.predict(X_test)\n",
    "        for k in range(0,len(y_test)-1):\n",
    "            if y_test[k] != y_pred[k]:\n",
    "                err_0[i,j] = err_0[i,j] + 1\n",
    "        err_0[i,j]= err_0[i,j]/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Viewing the base\n",
    "err_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# err is the method in the paper with bagging\n",
    "cases = len(df_to_use)\n",
    "err = np.zeros((i_max,j_max))\n",
    "for i in range(i_max):\n",
    "    \n",
    "    for j in range(j_max):\n",
    "        print(i, j, end=\"\\r\")\n",
    "        df_to_use_sample = df_to_use.sample(n=cases, replace=True)\n",
    "        X = df_to_use_sample.values[:, 1:35]\n",
    "        Y = df_to_use_sample.values[:, 0]#.astype('category')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3) # 70% training and 30% test\n",
    "        X_train = X_train.astype(float)\n",
    "        X_test = X_test.astype(float)\n",
    "        # replace missing values with mean (after train/test split)\n",
    "        # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "        col_means = np.nanmean(X_train, axis=0)\n",
    "        inds = np.where(np.isnan(X_train))\n",
    "        X_train[inds] = np.take(col_means, inds[1])\n",
    "        ### Rounding, because all values are categorical, not numerical\n",
    "        col_means = np.round(np.nanmean(X_test, axis=0))\n",
    "        inds = np.where(np.isnan(X_test))\n",
    "        X_test[inds] = np.take(col_means, inds[1])\n",
    "        # Create Decision Tree classifer object\n",
    "        clf = DecisionTreeClassifier()\n",
    "        # Train Decision Tree Classifer\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        #Predict the response for test dataset\n",
    "        y_pred = clf.predict(X_test)\n",
    "        for k in range(0,len(y_test)-1):\n",
    "            if y_test[k] != y_pred[k]:\n",
    "                err[i,j] = err[i,j] + 1\n",
    "        err[i,j]= err[i,j]/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Viewing err\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The paper found 14% for err0 and 10% for err, which is the improvement from bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "print(err_0.mean())\n",
    "plt.hist(err_0, bins = 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "print(err.mean())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.concatenate(err_0), bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.concatenate(err))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.hist(np.concatenate(err_0), color='tab:red', alpha=0.5, label='base - avg: {}'.format(round(err_0.mean(), 3)))\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.hist(np.concatenate(err),alpha=0.5, label='bagging - avg: {}'.format(round(err.mean(), 3)))\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.legend(loc='upper right')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
